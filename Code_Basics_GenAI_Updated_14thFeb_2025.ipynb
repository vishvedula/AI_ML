{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxulIQ8uwmRo2YP+032/Dt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishvedula/AI_ML/blob/main/Code_Basics_GenAI_Updated_14thFeb_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40VuMuCwhXeE"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pyngrok"
      ],
      "metadata": {
        "id": "SOKj8MXYjjrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2sNsmufdsJfwgVDdkH2GN70f2HH_7o7N1C3A7EN3mFhhmNpNQ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYPHRBQMjzKX",
        "outputId": "2a662627-119c-430c-9470-2e6f9de94f74"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "OUwMuuGJ0DZF"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"Hello, Streamlit on Colab!\")\n",
        "st.write(\"This is a basic Streamlit app running on Google Colab.\")\n",
        "\n",
        "number = st.slider(\"Pick a number\", 0, 100)\n",
        "st.write(f\"You selected: {number}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF3Hlrdmj4B3",
        "outputId": "36be8bed-aa1b-4621-995d-06529a21ba26"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup streamlit run app.py --server.port 8501 --server.enableCORS false --server.enableXsrfProtection false > logs.txt 2>&1 &"
      ],
      "metadata": {
        "id": "h1d9hpCLkARa"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Start Streamlit in the background\n",
        "#!nohup streamlit run genai.py --server.port 8501 --server.enableCORS false --server.enableXsrfProtection false > logs.txt 2>&1 &\n",
        "\n",
        "# Expose Streamlit on port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Streamlit App URL:\", public_url)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnL_AqUAkGgB",
        "outputId": "b055f1ee-1aa9-41c9-cd3f-157909991c05"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit App URL: NgrokTunnel: \"https://9bbc-35-190-178-91.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "ejdDUdcAkjMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-generativeai"
      ],
      "metadata": {
        "id": "UkQ4r7bRmNWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q InstructorEmbedding\n",
        "!pip install -q transformers sentence-transformers"
      ],
      "metadata": {
        "id": "eYrahO1hpVlp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U InstructorEmbedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLDfw1gxq6EO",
        "outputId": "69b93f84-2000-4beb-d126-2f4a58bc28a2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: InstructorEmbedding in /usr/local/lib/python3.11/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install HuggingFaceEmbeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zel2pZ6ntCsr",
        "outputId": "4fd61c93-3fa2-449d-f083-8874d06de463"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement HuggingFaceEmbeddings (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for HuggingFaceEmbeddings\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y InstructorEmbedding sentence-transformers langchain\n",
        "!pip install -U InstructorEmbedding sentence-transformers langchain\n"
      ],
      "metadata": {
        "id": "S4IAYfqGrfRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U langchain-community"
      ],
      "metadata": {
        "id": "JpTDYBjcr003"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "id": "e5HP5aqLvz9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile gen_ai.py\n",
        "from langchain.vectorstores import FAISS\n",
        "#from langchain.llms import GooglePalm #deprecated\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings,HuggingFaceEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()  # take environment variables from .env (especially openai api key)\n",
        "\n",
        "# Create Google Palm LLM model\n",
        "#llm = GooglePalm(google_api_key=os.getenv('GOOGLE_API_TOKEN'), temperature=0.1)\n",
        "#llm = GooglePalm(google_api_key='AIzaSyAYOAcj3yCc-98jlz1TJn0ALlMQXBzR_g4', temperature=0.1)\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=os.getenv('AIzaSyC6MSOEyhOhJS-I3YM4tUcmy6ClWiEOM5g'))#'AIzaSyAYOAcj3yCc-98jlz1TJn0ALlMQXBzR_g4')\n",
        "\n",
        "\n",
        "# # Initialize instructor embeddings using the Hugging Face model\n",
        "# instructor_embeddings = HuggingFaceInstructEmbeddings(\n",
        "#     model_name=\"hkunlp/instructor-large\",\n",
        "#     encode_kwargs={\"use_auth_token\": False}  # Disable token requirement\n",
        "# )\n",
        "\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "instructor_embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
        "\n",
        "vectordb_file_path = \"faiss_index\"\n",
        "\n",
        "def create_vector_db():\n",
        "    # Load data from FAQ sheet\n",
        "    loader = CSVLoader(file_path='codebasics_faqs.csv',encoding=\"ISO-8859-1\", source_column=\"prompt\")\n",
        "    data = loader.load()\n",
        "\n",
        "    # Create a FAISS instance for vector database from 'data'\n",
        "    vectordb = FAISS.from_documents(documents=data,\n",
        "                                    embedding=instructor_embeddings)\n",
        "\n",
        "    # Save vector database locally\n",
        "    vectordb.save_local(vectordb_file_path)\n",
        "\n",
        "\n",
        "def get_qa_chain():\n",
        "    # Load the vector database from the local folder\n",
        "    vectordb = FAISS.load_local(vectordb_file_path, instructor_embeddings, allow_dangerous_deserialization=True  # Allow pickle loading\n",
        ")\n",
        "\n",
        "    # Create a retriever for querying the vector database\n",
        "    retriever = vectordb.as_retriever(score_threshold=0.7)\n",
        "\n",
        "    prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only.\n",
        "    In the answer try to provide as much text as possible from \"response\" section in the source document context without making much changes.\n",
        "    If the answer is not found in the context, kindly state \"I don't know.\" Don't try to make up an answer.\n",
        "\n",
        "    CONTEXT: {context}\n",
        "\n",
        "    QUESTION: {question}\"\"\"\n",
        "\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                        chain_type=\"stuff\",\n",
        "                                        retriever=retriever,\n",
        "                                        input_key=\"query\",\n",
        "                                        return_source_documents=True,\n",
        "                                        chain_type_kwargs={\"prompt\": PROMPT})\n",
        "\n",
        "    return chain\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_vector_db()\n",
        "    chain = get_qa_chain()\n",
        "    #print(chain.invoke(\"Do you have javascript course?\"))\n",
        "    print(chain.invoke(\"Why should i trust codebasics?\"))"
      ],
      "metadata": {
        "id": "2P4jGZRJmCSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup streamlit run gen_ai.py --server.port 8501 --server.enableCORS false --server.enableXsrfProtection false > logs.txt 2>&1 &\n"
      ],
      "metadata": {
        "id": "iFhh55Qu0GAt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyngrok import ngrok\n",
        "#ngrok.kill()\n",
        "# Start Streamlit in the background\n",
        "!nohup streamlit run gen_ai.py --server.port 8501 --server.enableCORS false --server.enableXsrfProtection false > logs.txt 2>&1 &\n",
        "\n",
        "# Expose Streamlit on port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Streamlit App URL:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv2tUVJy0Lw9",
        "outputId": "ca70b3e5-1dc8-4e4e-84ee-f337db8b70ce"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit App URL: NgrokTunnel: \"https://a20e-35-190-178-91.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "QHwtlav4C4P3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx"
      ],
      "metadata": {
        "id": "ipFGzFqiVw1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(os.getcwd())  # Check current working directory\n",
        "print(os.path.exists(\"sample_document_with_comments.docx\"))\n",
        "with open(\"sample_document_with_comments.docx\", \"rb\") as f:\n",
        "    print(\"File opened successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzxgYdF2Wxyd",
        "outputId": "f069c7f2-e766-4ef9-850b-31d72c3c3118"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "True\n",
            "File opened successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "import os\n",
        "\n",
        "def extract_comments_from_docx(file_path):\n",
        "    try:\n",
        "        # Check if file exists\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "\n",
        "        # Load the document\n",
        "        doc = Document(file_path)\n",
        "        comments = []\n",
        "\n",
        "        # Process each paragraph\n",
        "        for i, paragraph in enumerate(doc.paragraphs):\n",
        "            # Get the paragraph text\n",
        "            text = paragraph.text\n",
        "\n",
        "            # Check if there are any comments in the paragraph text\n",
        "            if '[Comment:' in text:\n",
        "                # Extract comment using string manipulation for this specific format\n",
        "                start_idx = text.find('[Comment:')\n",
        "                end_idx = text.find(']', start_idx)\n",
        "                if end_idx != -1:\n",
        "                    comment_text = text[start_idx+9:end_idx].strip()\n",
        "\n",
        "                    # Split the comment text to separate reviewer\n",
        "                    parts = comment_text.split(' by ')\n",
        "                    if len(parts) == 2:\n",
        "                        comment_content = parts[0].strip()\n",
        "                        reviewer = parts[1].strip()\n",
        "\n",
        "                        comments.append({\n",
        "                            'paragraph_number': i + 1,\n",
        "                            'paragraph_text': text.replace(text[start_idx:end_idx+1], '').strip(),\n",
        "                            'reviewer': reviewer,\n",
        "                            'comment': comment_content\n",
        "                        })\n",
        "\n",
        "        return comments\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def print_comments(comments):\n",
        "    if not comments:\n",
        "        print(\"No comments found or error processing the document.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nExtracted Comments:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, comment in enumerate(comments, 1):\n",
        "        print(f\"\\nComment {i}:\")\n",
        "        print(f\"Paragraph {comment['paragraph_number']}:\")\n",
        "        print(f\"Text: {comment['paragraph_text']}\")\n",
        "        print(f\"Reviewer: {comment['reviewer']}\")\n",
        "        print(f\"Comment: {comment['comment']}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "def main():\n",
        "    #file_path = \"sample_document_with_comments.docx\"\n",
        "    file_path = \"openai_with_comments.docx\"\n",
        "    comments = extract_comments_from_docx(file_path)\n",
        "    print_comments(comments)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHquSMlfUvYE",
        "outputId": "1ff05a42-35dd-456a-c157-2e082554ac27"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No comments found or error processing the document.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "file_path = \"sample_document_with_comments.docx\"\n",
        "\n",
        "if zipfile.is_zipfile(file_path):\n",
        "    print(\"Valid .docx file\")\n",
        "else:\n",
        "    print(\"File is not a valid .docx (it might be a .doc or corrupted)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL8Iy2DGX9B0",
        "outputId": "84aedf8c-eff7-4544-b364-7c5de9a7d579"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid .docx file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "\n",
        "# Create a new Word document\n",
        "doc = Document()\n",
        "\n",
        "# Add text paragraphs\n",
        "p1 = doc.add_paragraph(\"This is a sample document with comments.\")\n",
        "p2 = doc.add_paragraph(\"Another line that needs review.\")\n",
        "p3 = doc.add_paragraph(\"Final paragraph with some notes.\")\n",
        "\n",
        "# Manually add comment indicators (since python-docx doesn't support comments)\n",
        "p1.runs[0].bold = True\n",
        "p1.runs[0].font.size = Pt(12)\n",
        "p1.add_run(\" [Comment: This is a comment on the first sentence by Reviewer 1]\")\n",
        "\n",
        "p2.runs[0].italic = True\n",
        "p2.add_run(\" [Comment: Consider rewording this part for clarity by Reviewer 2]\")\n",
        "\n",
        "p3.runs[0].underline = True\n",
        "p3.add_run(\" [Comment: Final paragraph looks good, but check grammar by Reviewer 3]\")\n",
        "\n",
        "# Save the document\n",
        "file_path = \"openAI_document_with_comments.docx\"\n",
        "#file_path = \"openai_with_comments.docx\"\n",
        "doc.save(file_path)\n",
        "\n",
        "# Return the file path\n",
        "file_path\n"
      ],
      "metadata": {
        "id": "WvEM1Iakb9v0",
        "outputId": "059faaab-8f6d-4651-f7df-95ceff3f0a62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'openAI_document_with_comments.docx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "from docx.shared import Pt, RGBColor, Inches\n",
        "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
        "\n",
        "def create_sample_document():\n",
        "    # Create a new document\n",
        "    doc = Document()\n",
        "\n",
        "    # Add a title\n",
        "    title = doc.add_heading('Sample Document with Comments', 0)\n",
        "    title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "\n",
        "    # Add author and date\n",
        "    doc.add_paragraph('Author: John Doe')\n",
        "    doc.add_paragraph('Date: February 13, 2025')\n",
        "\n",
        "    # Add a section with normal text and a comment\n",
        "    doc.add_paragraph('This is the first paragraph of our document. It contains some important information that needs to be reviewed. [Comment by Reviewer 1: Please verify this information]')\n",
        "\n",
        "    # Add a heading for key points\n",
        "    doc.add_heading('Key Points:', level=1)\n",
        "\n",
        "    # Add bullet points with comments\n",
        "    doc.add_paragraph(\n",
        "        '• First key point about the project [Comment by Reviewer 2: Consider adding more details here]',\n",
        "        style='List Bullet'\n",
        "    )\n",
        "    doc.add_paragraph(\n",
        "        '• Second key point with additional context',\n",
        "        style='List Bullet'\n",
        "    )\n",
        "\n",
        "    # Add analysis section with colored text\n",
        "    doc.add_heading('Analysis:', level=1)\n",
        "    paragraph = doc.add_paragraph()\n",
        "    run = paragraph.add_run('This section contains our analysis of the situation. ')\n",
        "    run.font.color.rgb = RGBColor(0, 0, 139)  # Dark blue color\n",
        "    run = paragraph.add_run('[Comment by Reviewer 3: The analysis needs more quantitative data]')\n",
        "    run.font.color.rgb = RGBColor(255, 0, 0)  # Red color for comment\n",
        "\n",
        "    # Add a conclusion with comment\n",
        "    doc.add_heading('Conclusion:', level=1)\n",
        "    doc.add_paragraph('Based on our findings, we recommend proceeding with the proposed changes. [Comment by Reviewer 1: Good conclusion, but maybe add timeline]')\n",
        "\n",
        "    # Add a table\n",
        "    doc.add_heading('Project Timeline:', level=1)\n",
        "    table = doc.add_table(rows=3, cols=2)\n",
        "    table.style = 'Table Grid'\n",
        "\n",
        "    # Add content to table\n",
        "    cells = table.rows[0].cells\n",
        "    cells[0].text = 'Phase'\n",
        "    cells[1].text = 'Duration'\n",
        "\n",
        "    cells = table.rows[1].cells\n",
        "    cells[0].text = 'Planning'\n",
        "    cells[1].text = '2 months'\n",
        "\n",
        "    cells = table.rows[2].cells\n",
        "    cells[0].text = 'Implementation'\n",
        "    cells[1].text = '4 months'\n",
        "\n",
        "    # Save the document\n",
        "    output_file = 'openai_with_comments.docx'\n",
        "    doc.save(output_file)\n",
        "    return output_file\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        output_file = create_sample_document()\n",
        "        print(f\"Document successfully created: {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating document: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "tZygHjmHcm4l",
        "outputId": "80347184-7319-4250-fea6-bd65874ca072",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document successfully created: openai_with_comments.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "import os\n",
        "\n",
        "def extract_comments_from_docx(file_path):\n",
        "    try:\n",
        "        # Check if file exists\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "\n",
        "        # Load the document\n",
        "        doc = Document(file_path)\n",
        "        comments = []\n",
        "\n",
        "        # Process each paragraph\n",
        "        for i, paragraph in enumerate(doc.paragraphs):\n",
        "            # Get the paragraph text\n",
        "            text = paragraph.text\n",
        "\n",
        "            # Check if there are any comments in the paragraph text\n",
        "            if '[Comment by' in text:\n",
        "                # Extract comment using string manipulation for this specific format\n",
        "                start_idx = text.find('[Comment by')\n",
        "                end_idx = text.find(']', start_idx)\n",
        "                if end_idx != -1:\n",
        "                    comment_full = text[start_idx+1:end_idx]  # Remove the brackets\n",
        "\n",
        "                    # Split into parts: \"Comment by Reviewer X: actual comment\"\n",
        "                    try:\n",
        "                        # Split at the colon to separate the reviewer from the comment\n",
        "                        reviewer_part, comment_content = comment_full.split(': ', 1)\n",
        "                        # Extract reviewer name\n",
        "                        reviewer = reviewer_part.replace('Comment by ', '')\n",
        "\n",
        "                        comments.append({\n",
        "                            'paragraph_number': i + 1,\n",
        "                            'paragraph_text': text[:start_idx].strip(),  # Text before the comment\n",
        "                            'reviewer': reviewer,\n",
        "                            'comment': comment_content.strip()\n",
        "                        })\n",
        "                    except ValueError:\n",
        "                        continue  # Skip if the format doesn't match expected\n",
        "\n",
        "        return comments\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def print_comments(comments):\n",
        "    if not comments:\n",
        "        print(\"No comments found or error processing the document.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nExtracted Comments:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, comment in enumerate(comments, 1):\n",
        "        print(f\"\\nComment {i}:\")\n",
        "        print(f\"Paragraph {comment['paragraph_number']}:\")\n",
        "        print(f\"Text: {comment['paragraph_text']}\")\n",
        "        print(f\"Reviewer: {comment['reviewer']}\")\n",
        "        print(f\"Comment: {comment['comment']}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "def main():\n",
        "    file_path = \"openai_with_comments.docx\"\n",
        "    comments = extract_comments_from_docx(file_path)\n",
        "    print_comments(comments)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "l91dTAWpdoHa",
        "outputId": "e6911749-6cfc-4e57-853f-5f5ef97b0559",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted Comments:\n",
            "--------------------------------------------------\n",
            "\n",
            "Comment 1:\n",
            "Paragraph 4:\n",
            "Text: This is the first paragraph of our document. It contains some important information that needs to be reviewed.\n",
            "Reviewer: Reviewer 1\n",
            "Comment: Please verify this information\n",
            "--------------------------------------------------\n",
            "\n",
            "Comment 2:\n",
            "Paragraph 6:\n",
            "Text: • First key point about the project\n",
            "Reviewer: Reviewer 2\n",
            "Comment: Consider adding more details here\n",
            "--------------------------------------------------\n",
            "\n",
            "Comment 3:\n",
            "Paragraph 9:\n",
            "Text: This section contains our analysis of the situation.\n",
            "Reviewer: Reviewer 3\n",
            "Comment: The analysis needs more quantitative data\n",
            "--------------------------------------------------\n",
            "\n",
            "Comment 4:\n",
            "Paragraph 11:\n",
            "Text: Based on our findings, we recommend proceeding with the proposed changes.\n",
            "Reviewer: Reviewer 1\n",
            "Comment: Good conclusion, but maybe add timeline\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}