{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2AZyzou7lH0h7HyvrduQr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishvedula/AI_ML/blob/main/Code_Basics_GenAI_Updated_13thFeb_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40VuMuCwhXeE"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pyngrok"
      ],
      "metadata": {
        "id": "SOKj8MXYjjrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2sNsmufdsJfwgVDdkH2GN70f2HH_7o7N1C3A7EN3mFhhmNpNQ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYPHRBQMjzKX",
        "outputId": "7a4ed1fb-8391-4948-ed2a-871f6427850b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "OUwMuuGJ0DZF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"Hello, Streamlit on Colab!\")\n",
        "st.write(\"This is a basic Streamlit app running on Google Colab.\")\n",
        "\n",
        "number = st.slider(\"Pick a number\", 0, 100)\n",
        "st.write(f\"You selected: {number}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF3Hlrdmj4B3",
        "outputId": "69006362-c74c-45b1-843a-ba937fe267dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup streamlit run app.py --server.port 8501 --server.enableCORS false --server.enableXsrfProtection false > logs.txt 2>&1 &"
      ],
      "metadata": {
        "id": "h1d9hpCLkARa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Start Streamlit in the background\n",
        "#!nohup streamlit run genai.py --server.port 8501 --server.enableCORS false --server.enableXsrfProtection false > logs.txt 2>&1 &\n",
        "\n",
        "# Expose Streamlit on port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Streamlit App URL:\", public_url)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnL_AqUAkGgB",
        "outputId": "5f333f93-7bd4-44bb-afc4-7128f6eccff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit App URL: NgrokTunnel: \"https://13fc-35-221-39-141.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "ejdDUdcAkjMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-generativeai"
      ],
      "metadata": {
        "id": "UkQ4r7bRmNWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q InstructorEmbedding\n",
        "!pip install -q transformers sentence-transformers"
      ],
      "metadata": {
        "id": "eYrahO1hpVlp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U InstructorEmbedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLDfw1gxq6EO",
        "outputId": "92c9b637-97b2-4e65-dd3c-37de776a5dc4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: InstructorEmbedding in /usr/local/lib/python3.11/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install HuggingFaceEmbeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zel2pZ6ntCsr",
        "outputId": "2d46c167-0894-4208-8638-ec2dc3af94c0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement HuggingFaceEmbeddings (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for HuggingFaceEmbeddings\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y InstructorEmbedding sentence-transformers langchain\n",
        "!pip install -U InstructorEmbedding sentence-transformers langchain\n"
      ],
      "metadata": {
        "id": "S4IAYfqGrfRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U langchain-community"
      ],
      "metadata": {
        "id": "JpTDYBjcr003"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "id": "e5HP5aqLvz9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from langchain.vectorstores import FAISS\n",
        "#from langchain.llms import GooglePalm #deprecated\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings,HuggingFaceEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()  # take environment variables from .env (especially openai api key)\n",
        "\n",
        "# Create Google Palm LLM model\n",
        "#llm = GooglePalm(google_api_key=os.getenv('GOOGLE_API_TOKEN'), temperature=0.1)\n",
        "#llm = GooglePalm(google_api_key='AIzaSyAYOAcj3yCc-98jlz1TJn0ALlMQXBzR_g4', temperature=0.1)\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=os.getenv('Gemini API'))#'AIzaSyAYOAcj3yCc-98jlz1TJn0ALlMQXBzR_g4')\n",
        "\n",
        "\n",
        "# # Initialize instructor embeddings using the Hugging Face model\n",
        "# instructor_embeddings = HuggingFaceInstructEmbeddings(\n",
        "#     model_name=\"hkunlp/instructor-large\",\n",
        "#     encode_kwargs={\"use_auth_token\": False}  # Disable token requirement\n",
        "# )\n",
        "\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "instructor_embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
        "\n",
        "vectordb_file_path = \"faiss_index\"\n",
        "\n",
        "def create_vector_db():\n",
        "    # Load data from FAQ sheet\n",
        "    loader = CSVLoader(file_path='codebasics_faqs.csv',encoding=\"ISO-8859-1\", source_column=\"prompt\")\n",
        "    data = loader.load()\n",
        "\n",
        "    # Create a FAISS instance for vector database from 'data'\n",
        "    vectordb = FAISS.from_documents(documents=data,\n",
        "                                    embedding=instructor_embeddings)\n",
        "\n",
        "    # Save vector database locally\n",
        "    vectordb.save_local(vectordb_file_path)\n",
        "\n",
        "\n",
        "def get_qa_chain():\n",
        "    # Load the vector database from the local folder\n",
        "    vectordb = FAISS.load_local(vectordb_file_path, instructor_embeddings, allow_dangerous_deserialization=True  # Allow pickle loading\n",
        ")\n",
        "\n",
        "    # Create a retriever for querying the vector database\n",
        "    retriever = vectordb.as_retriever(score_threshold=0.7)\n",
        "\n",
        "    prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only.\n",
        "    In the answer try to provide as much text as possible from \"response\" section in the source document context without making much changes.\n",
        "    If the answer is not found in the context, kindly state \"I don't know.\" Don't try to make up an answer.\n",
        "\n",
        "    CONTEXT: {context}\n",
        "\n",
        "    QUESTION: {question}\"\"\"\n",
        "\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                        chain_type=\"stuff\",\n",
        "                                        retriever=retriever,\n",
        "                                        input_key=\"query\",\n",
        "                                        return_source_documents=True,\n",
        "                                        chain_type_kwargs={\"prompt\": PROMPT})\n",
        "\n",
        "    return chain\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_vector_db()\n",
        "    chain = get_qa_chain()\n",
        "    #print(chain.invoke(\"Do you have javascript course?\"))\n",
        "    print(chain.invoke(\"Why should i trust codebasics?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P4jGZRJmCSw",
        "outputId": "3c99a03a-a36b-4157-87ca-8283c41ed1b0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup streamlit run app.py --server.port 8501 --server.enableCORS false --server.enableXsrfProtection false > logs.txt 2>&1 &\n"
      ],
      "metadata": {
        "id": "iFhh55Qu0GAt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyngrok import ngrok\n",
        "#ngrok.kill()\n",
        "# Start Streamlit in the background\n",
        "!nohup streamlit run main.py --server.port 8501 --server.enableCORS false --server.enableXsrfProtection false > logs.txt 2>&1 &\n",
        "\n",
        "# Expose Streamlit on port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Streamlit App URL:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv2tUVJy0Lw9",
        "outputId": "50fd87b2-3623-4b40-81d3-9561baa77a09"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit App URL: NgrokTunnel: \"https://9ba1-34-75-13-152.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok http 8501\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-ZdjxKs13hF",
        "outputId": "df3ef040-af60-4324-e1ba-2705ee90127e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR:  authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\n",
            "ERROR:  You can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\n",
            "ERROR:  Read more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\n",
            "ERROR:  You can view your current agent sessions in the dashboard:\n",
            "ERROR:  https://dashboard.ngrok.com/agents\r\n",
            "ERROR:  \r\n",
            "ERROR:  ERR_NGROK_108\r\n",
            "ERROR:  https://ngrok.com/docs/errors/err_ngrok_108\r\n",
            "ERROR:  \n"
          ]
        }
      ]
    }
  ]
}