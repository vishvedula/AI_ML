{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvKBRUY7KYC1M2/JYa9tm3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishvedula/AI_ML/blob/main/Code_Basics_GenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40VuMuCwhXeE"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pyngrok"
      ],
      "metadata": {
        "id": "SOKj8MXYjjrL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f04dea8f-5aa4-41a5-8b62-fc602ba55111"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2sNsmufdsJfwgVDdkH2GN70f2HH_7o7N1C3A7EN3mFhhmNpNQ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYPHRBQMjzKX",
        "outputId": "8b846a86-5d91-4c83-c3cb-508c62316fb9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "OUwMuuGJ0DZF"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"Hello, Streamlit on Colab!\")\n",
        "st.write(\"This is a basic Streamlit app running on Google Colab.\")\n",
        "\n",
        "number = st.slider(\"Pick a number\", 0, 100)\n",
        "st.write(f\"You selected: {number}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF3Hlrdmj4B3",
        "outputId": "69006362-c74c-45b1-843a-ba937fe267dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup streamlit run app.py --server.port 8501 --server.enableCORS false --server.enableXsrfProtection false > logs.txt 2>&1 &"
      ],
      "metadata": {
        "id": "h1d9hpCLkARa"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Start Streamlit in the background\n",
        "#!nohup streamlit run genai.py --server.port 8501 --server.enableCORS false --server.enableXsrfProtection false > logs.txt 2>&1 &\n",
        "\n",
        "# Expose Streamlit on port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Streamlit App URL:\", public_url)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnL_AqUAkGgB",
        "outputId": "8b75ad9b-7088-4572-ef72-b0c3ea6be029"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit App URL: NgrokTunnel: \"https://d949-35-221-39-141.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "ejdDUdcAkjMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-generativeai"
      ],
      "metadata": {
        "id": "UkQ4r7bRmNWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q InstructorEmbedding\n",
        "!pip install -q transformers sentence-transformers"
      ],
      "metadata": {
        "id": "eYrahO1hpVlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U InstructorEmbedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLDfw1gxq6EO",
        "outputId": "73533132-a598-4f8a-937b-8c0b57f1b13d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: InstructorEmbedding in /usr/local/lib/python3.11/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install HuggingFaceEmbeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zel2pZ6ntCsr",
        "outputId": "32d5a892-253c-4701-ca38-307c005dd20b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement HuggingFaceEmbeddings (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for HuggingFaceEmbeddings\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y InstructorEmbedding sentence-transformers langchain\n",
        "!pip install -U InstructorEmbedding sentence-transformers langchain\n"
      ],
      "metadata": {
        "id": "S4IAYfqGrfRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U langchain-community"
      ],
      "metadata": {
        "id": "JpTDYBjcr003"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "id": "e5HP5aqLvz9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "#from langchain.llms import GooglePalm #deprecated\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings,HuggingFaceEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()  # take environment variables from .env (especially openai api key)\n",
        "\n",
        "# Create Google Palm LLM model\n",
        "#llm = GooglePalm(google_api_key=os.getenv('GOOGLE_API_TOKEN'), temperature=0.1)\n",
        "#llm = GooglePalm(google_api_key='AIzaSyAYOAcj3yCc-98jlz1TJn0ALlMQXBzR_g4', temperature=0.1)\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=os.getenv('GOOGLE_API_KEY'))#'AIzaSyAYOAcj3yCc-98jlz1TJn0ALlMQXBzR_g4')\n",
        "\n",
        "\n",
        "# # Initialize instructor embeddings using the Hugging Face model\n",
        "# instructor_embeddings = HuggingFaceInstructEmbeddings(\n",
        "#     model_name=\"hkunlp/instructor-large\",\n",
        "#     encode_kwargs={\"use_auth_token\": False}  # Disable token requirement\n",
        "# )\n",
        "\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "instructor_embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
        "\n",
        "vectordb_file_path = \"faiss_index\"\n",
        "\n",
        "def create_vector_db():\n",
        "    # Load data from FAQ sheet\n",
        "    loader = CSVLoader(file_path='codebasics_faqs.csv',encoding=\"ISO-8859-1\", source_column=\"prompt\")\n",
        "    data = loader.load()\n",
        "\n",
        "    # Create a FAISS instance for vector database from 'data'\n",
        "    vectordb = FAISS.from_documents(documents=data,\n",
        "                                    embedding=instructor_embeddings)\n",
        "\n",
        "    # Save vector database locally\n",
        "    vectordb.save_local(vectordb_file_path)\n",
        "\n",
        "\n",
        "def get_qa_chain():\n",
        "    # Load the vector database from the local folder\n",
        "    vectordb = FAISS.load_local(vectordb_file_path, instructor_embeddings, allow_dangerous_deserialization=True  # Allow pickle loading\n",
        ")\n",
        "\n",
        "    # Create a retriever for querying the vector database\n",
        "    retriever = vectordb.as_retriever(score_threshold=0.7)\n",
        "\n",
        "    prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only.\n",
        "    In the answer try to provide as much text as possible from \"response\" section in the source document context without making much changes.\n",
        "    If the answer is not found in the context, kindly state \"I don't know.\" Don't try to make up an answer.\n",
        "\n",
        "    CONTEXT: {context}\n",
        "\n",
        "    QUESTION: {question}\"\"\"\n",
        "\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                        chain_type=\"stuff\",\n",
        "                                        retriever=retriever,\n",
        "                                        input_key=\"query\",\n",
        "                                        return_source_documents=True,\n",
        "                                        chain_type_kwargs={\"prompt\": PROMPT})\n",
        "\n",
        "    return chain\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_vector_db()\n",
        "    chain = get_qa_chain()\n",
        "    #print(chain.invoke(\"Do you have javascript course?\"))\n",
        "    print(chain.invoke(\"Why should i trust codebasics?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P4jGZRJmCSw",
        "outputId": "b9b1d69c-1244-40f7-89f4-f29f1872eff0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'Why should i trust codebasics?', 'result': 'Till now 9000 + learners have benefitted from the quality of our courses. You can check the review section and also we have attached their LinkedIn profiles so that you can connect with them and ask directly.', 'source_documents': [Document(id='11ea08fd-2360-47e0-9c8c-3c17ab80b6ac', metadata={'source': 'Why should I trust Codebasics?', 'row': 1}, page_content='prompt: Why should I trust Codebasics?\\nresponse: Till now 9000 + learners have benefitted from the quality of our courses. You can check the review section and also we have attached their LinkedIn profiles so that you can connect with them and ask directly.'), Document(id='0ea91121-f8ae-4716-92b4-526627075ecc', metadata={'source': 'I\\x92m not sure if this course is good enough for me to invest some money. What can I do?', 'row': 20}, page_content='prompt: I\\x92m not sure if this course is good enough for me to invest some money. What can I do?\\nresponse: Don\\x92t worry. Many videos in this course are free so watch them to get an idea of the quality of teaching. Dhaval Patel (the course instructor) runs a popular data science YouTube channel called Codebasics. On that, you can watch his videos and read comments to get an idea of his teaching style'), Document(id='82839f8e-40fa-4b7c-8151-138e1d18925f', metadata={'source': 'I have never done programming and belong to a non-technical background. Can I take this course?', 'row': 24}, page_content='prompt: I have never done programming and belong to a non-technical background. Can I take this course?\\nresponse: Yes, this is the perfect course for anyone who has never done coding and wants to build a career in the IT/Data Analytics industry or just wants to perform better in their current job or business using data.'), Document(id='3e6ec011-7914-472e-bc9f-d1957bbb83dc', metadata={'source': 'Hi all, could someone please help me understand what I am writing wrong in this code?', 'row': 39}, page_content='prompt: Hi all, could someone please help me understand what I am writing wrong in this code?\\nresponse: Go to ChatGPT (https://chat.openai.com/) and type this command\\n\\nDebug me this code ( copy paste your code)\\n\\n\\nWhy I am suggesting this way is once you learn it you will love it. \\nAnd it will be SUPER USEFUL in your career when you get a data analyst jobs. Companies love folks who are capable of finding their answers on their own (Using Google, ChatGPT etc.)')]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup streamlit run main.py --server.port 8501 --server.enableCORS false --server.enableXsrfProtection false > logs.txt 2>&1 &\n"
      ],
      "metadata": {
        "id": "iFhh55Qu0GAt"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Start Streamlit in the background\n",
        "!nohup streamlit run main.py --server.port 8501 --server.enableCORS false --server.enableXsrfProtection false > logs.txt 2>&1 &\n",
        "\n",
        "# Expose Streamlit on port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Streamlit App URL:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv2tUVJy0Lw9",
        "outputId": "35820725-efe1-48f2-eb11-ec50385d7599"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit App URL: NgrokTunnel: \"https://bed5-35-221-39-141.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}